# -*- coding: utf-8 -*-
"""Migration-ETL-form_response.v1.1.0.0-40000.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n8HjTIufu4oRXlQu6d1cx-QO9xVZU08T
"""

import os
import re
import json
import pandas as pd
import numpy as np
from pandas import json_normalize
from google.cloud import storage
from tqdm import tqdm

import logs.logging_conf, logging
logger = logging.getLogger("gcs_to_openrefine.py")

"""## Mount Google Drive (if using service account keys)"""

#from google.colab import drive
#drive.mount('/content/drive')

"""## Set Env Variables"""

ENVIRONMENT = 'production'
#ENVIRONMENT = 'development'

service_account_key = "prod-gcs-colab@reach52-dataops.iam.gserviceaccount.com.json"
project_id = 'reach52-dataops'
country = "philippines"
stg_bucket_name = f"stage-r52_dhs-{country}"
fnl_bucket_name = f"final-r52_dhs-{country}"

os.environ['GOOGLE_APPLICATION_CREDENTIALS']=f"./secrets/{service_account_key}"
gcs_colab_secret = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')

#gcloud config set project {project_id}

"""## Authenticate using service account

"""

from oauth2client.service_account import ServiceAccountCredentials

scope = ['https://www.googleapis.com/auth/analytics',
        'https://www.googleapis.com/auth/analytics.edit']
creds = ServiceAccountCredentials.from_json_keyfile_name(gcs_colab_secret, scope)

#client = gspread.authorize(creds)

"""## Extract

### Iterate GCS file in Staging Directory
"""

def list_staging_files(bucket_name, prefix_url):
  storage_client = storage.Client()

  #blobs = storage_client.list_blobs(bucket_name, prefix="_Stage/form-response/")
  blobs = storage_client.list_blobs(bucket_name, prefix=prefix_url)
  #blob_list = list(blobs)
  blob_list = []

  for blob in blobs:
    blob_list.append(blob.name)

  #return blob_list[1:]
  return blob_list[1:]

def extract_gcs_blob(bucket_name, blob_path):
  print('extracting blob...')
  
  storage_client = storage.Client()
  bucket = storage_client.get_bucket(bucket_name)

  blob = bucket.blob(blob_path)
  data = blob.download_as_string().decode('utf-8')

  json_data = json.loads(data)
  data_df = json_normalize(json_data)
  return data_df

"""### Read file"""

def extract_file_id(blob_path):  
  file_id = blob_path.split(".")[-2]
  print('extracting file id...', file_id)
  return file_id

def extract_file_name(blob_path):
  full_path = blob_path.split('/')[-1].split('.')
  file_name = '.'.join(full_path[0:-1])
  #file_name = '_Stage/form-response/IND/form_response.reach52MasterLiveIND1.android.ap-south1.2022-03-28.02:03:25.00657ee2-9861-4091-9164-766d61f6752c.json'
  #print('extracting file name...', file_name)
  return file_name

"""  ## Transform"""

def set_data_type(data):
  if  'residentid' in data.lower():
    dtype = 'personal_indentifier'
  elif 'question' in data:
    dtype = 'question'
  elif 'othertag' in data.lower():
    dtype = 'tag'
  elif 'questiontag' in data.lower():
    dtype = 'tag'
  else: 
    dtype = 'metadata'
  return dtype

"""### Convert to EAV model"""

def build_eav_model(data_df, file_id):
  print('building eav model...')

  for i in (range(len(data_df))):
    eav_df = pd.DataFrame()
    eav_df['question_value'] = data_df.iloc[i].str.strip().replace('\n', '', regex=True)
    eav_df['question_key'] = data_df.keys().str.strip()
    eav_df['question_order'] = pd.to_numeric(np.arange(len(data_df.keys())))
    eav_df['file_id'] = file_id
    
    created_at_header = '' 
    #print(data_df)
    if '_meta.createdDate.$date' in data_df.columns:
      created_at_header = '_meta.createdDate.$date' 
    else:
      created_at_header = '_meta.createdTimestamp.$date' 
    
    #eav_df['created_at'] = data_df['_meta.createdDate.$date'].values[0]
    #eav_df['created_at'] = data_df['_meta.createdTimestamp.$date'].values[0]
    eav_df['created_at'] = data_df[created_at_header].values[0]
    eav_df['updated_at'] = np.nan
    eav_df['source_id'] = data_df['_id.$oid'].values[0]
    eav_df['data_type'] = eav_df['question_key'].apply(lambda x: set_data_type(x))

    eav_df = eav_df.reset_index(drop=True)
    eav_df = eav_df[['question_order'
                          ,'source_id'
                          ,'file_id'
                          ,'created_at'
                          ,'updated_at'
                          ,'data_type'
                          ,'question_key'
                          ,'question_value']]

  return eav_df

def extract_reference_array(question_key):
  reference_array = [item for item in question_key.split(".") if "[" in item] 
  first_array = reference_array[0]
  content_code_indx = re.search(r"\[([A-Za-z0-9_]+)\]", first_array)
  return content_code_indx.group(1)

def extract_reference_field(datas):
  result_df = pd.DataFrame()
  result_df['question_value'] = datas['question_value']
  result_df['question_key'] = datas['question_key'].apply(lambda x: x.split(".")[-1])
  result_df['reference_array'] = datas['question_key'].apply(lambda x: extract_reference_array(x) )
  return result_df

def locate_reference(question_code_df, indx):
  return question_code_df.index[question_code_df['reference_array'] == indx].values[0]

def link_reference(content_reference_df, question_code_df):
  print('linking references to content...')

  concatenated_df = pd.DataFrame()
  appended_data = []  
  for indx, value in content_reference_df.iterrows():
  
      reference_index = locate_reference(question_code_df, value['reference_array'][0])

      references_df = pd.DataFrame()
      references_df[['reference_type','reference_value']] = content_reference_df[['question_key', 'question_value']].loc[[reference_index]].reset_index(drop=True)
      references_df['question_order'] = indx

      appended_data.append(references_df)

  return pd.concat(appended_data,ignore_index=True)

def final_cleanup(merged_df, main_reference_key):
  print('cleaning final output...')

  merged_df['question_key'] = merged_df['question_key'].apply(lambda x: x.split(".")[-1])
  merged_df['reference_value'] = np.where(merged_df['question_key'] == main_reference_key, np.nan, merged_df['reference_value'])
  merged_df['reference_type'] = np.where(merged_df['question_key'] == main_reference_key, np.nan, merged_df['reference_type'])
  merged_df = merged_df.drop(merged_df[(merged_df['question_value'].isna()) | (merged_df['question_value'] == '') ].index)
  merged_df.reset_index(drop=True)
  return merged_df

def upload_gcs_blob(bucket_name, final_df, fnl_folder_type, collection_type, file_name): #PROD
#def upload_gcs_blob(final_df, fnl_folder_type, collection_type, file_name): #DEV
  print('uploading blob to gcs...')
  client = storage.Client()
  bucket = client.get_bucket(bucket_name)
  #result = bucket.blob(f'{fnl_folder_type}/{collection_type}/{file_name}.csv').upload_from_string(final_df.to_csv(index=False), 'text/csv') #DEV
  result = bucket.blob(f'{collection_type}/{file_name}.csv').upload_from_string(final_df.to_csv(index=False), 'text/csv') #PROD

def blob_exists(bucket_name, filename):
   client = storage.Client()
   bucket = client.get_bucket(bucket_name)
   blob = bucket.blob(filename)
   return blob.exists()

#blob_exists('stage-r52_dhs-philippines', 'form-response/form_response.reach52MasterLiveIND1.android.ap-south1.2022-00-06.03:00:00.205f82e3-664f-4d68-9ff9-2610d04fcd82.json')
#blob_exists('stage-r52_dhs-philippines','form-response/form_response.reach52MasterLiveIND1.android.ap-south1.2022-00-06.03:00:00.20865411-8e25-4233-b29e-69eb57eaae3d.json')

def move_blob(source_bucket_name, source_file_name, destination_bucket_name, destination_file_name):
#def move_blob():

  #'stage-r52_dhs-philippines/form-response/form_response.reach52MasterLiveIND1.android.ap-south1.2022-00-06.03:00:00.205f82e3-664f-4d68-9ff9-2610d04fcd82.json'
  #source_bucket_name = 'stage-r52_dhs-philippines'
  #source_file_name = 'form-response/form_response.reach52MasterLiveIND1.android.ap-south1.2022-00-06.03:00:00.35cfe521-7b3e-4a21-8951-be3957120928x.json'
  #destination_bucket_name = 'stage-r52_dhs-philippines'
  #destination_file_name = 'form-response/_archived/form_response.reach52MasterLiveIND1.android.ap-south1.2022-00-06.03:00:00.35cfe521-7b3e-4a21-8951-be3957120928x.json'

  print('moving blob to archived...')
  storage_client = storage.Client()

  source_bucket = storage_client.bucket(source_bucket_name)
  source_blob = source_bucket.blob(source_file_name)
  destination_bucket = storage_client.bucket(destination_bucket_name)

  if blob_exists(source_bucket_name, source_file_name):
    blob_copy = source_bucket.copy_blob(
        source_blob, destination_bucket, destination_file_name
    )

    print(
        "Blob {} in bucket {} copied to blob {} in bucket {}.".format(
            source_blob.name,
            source_bucket.name,
            blob_copy.name,
            destination_bucket.name,
        )
    )

  source_bucket.delete_blob(source_file_name)

def main():

  stg_collection_type = 'form-response'
  fnl_collection_type = 'form-response'
  stg_folder_type = '_Stage'
  fnl_folder_type = '_Final'

  content_data_type = 'question' #form-response
  main_reference_key = 'questionCode' #form-response

  prefix = f"{stg_collection_type}" #PROD

  #staging_files = list_staging_files(bucket_name, prefix)
  staging_files = list_staging_files(stg_bucket_name, prefix)
  # staging_files = [
      #'_Stage/form-response/IND/form_response.reach52MasterLiveIND1.android.ap-south1.2022-03-28.02:03:25.003e1e13-3870-4012-b2c1-ba89a7c16046.json',
      #'_Stage/form-response/IND/form_response.reach52MasterLiveIND1.android.ap-south1.2022-03-28.02:03:25.00657ee2-9861-4091-9164-766d61f6752c.json'
      # 'stage-r52_dhs-philippines/form-response/form_response.reach52MasterLiveIND1.android.ap-south1.2022-00-06.03:00:00.2197fecc-4faf-42b3-8752-148338e1914a.json',
      # 'stage-r52_dhs-philippines/form-response/form_response.reach52MasterLiveIND1.android.ap-south1.2022-00-06.03:00:00.20865411-8e25-4233-b29e-69eb57eaae3d.json'
      # ]

  #for indx, staging_file in enumerate(staging_files[9658:40000]): 
  for indx, staging_file in enumerate(staging_files): 
    print(f'processing...{indx}/{len(staging_files)}')
    print(f'staging_file... ', staging_file)
    
    file_name = extract_file_name(staging_file)
    
    file_id = extract_file_id(staging_file)
    
    data_df = extract_gcs_blob(stg_bucket_name, staging_file)

    eav_df = build_eav_model(data_df, file_id)

    content_df = eav_df[['question_key','question_value']][eav_df['data_type'] == content_data_type ]

    content_reference_df = extract_reference_field(content_df)

    main_reference_df = content_reference_df[content_reference_df['question_key'] ==  main_reference_key ]

    reference_df = link_reference(content_reference_df, main_reference_df)

    merged_df = pd.merge(eav_df, reference_df, on='question_order', how='outer')

    final_df = final_cleanup(merged_df, main_reference_key)
    
    #upload_gcs_blob(final_df, fnl_folder_type, fnl_collection_type, file_name) #DEV
    upload_gcs_blob(fnl_bucket_name, final_df, fnl_folder_type, fnl_collection_type, file_name) #PROD

    move_blob(stg_bucket_name, staging_file, stg_bucket_name, f"{stg_collection_type}/_archived/{file_name}.json")
    print('=============')

if __name__ == "__main__":
  main()
